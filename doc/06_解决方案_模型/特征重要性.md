在 LightGBM 中，获取特征的重要性（特征权重）是非常直观的，可以通过 `booster_.feature_importance()` 方法获取每个特征的权重（重要性）。然后，您可以根据这些特征的重要性进行筛选，只保留那些权重大于 0.05 的特征。

### 示例代码

假设您已经训练了一个 LightGBM 模型，并且想要筛选出特征权重大于 0.05 的特征。

#### 1. 获取特征权重

首先，我们需要训练一个 LightGBM 模型并获得特征的重要性。

#### 2. 筛选出权重大于 0.05 的特征

```python
import lightgbm as lgb
import pandas as pd
import numpy as np

# 假设您已经训练了一个LightGBM模型 lgb_model
# 例如：
# lgb_model = lgb.LGBMClassifier()
# lgb_model.fit(X_train, y_train)

# 获取特征的重要性
feature_importances = lgb_model.booster_.feature_importance(importance_type='split')  # 使用'gain'也可以

# 获取特征名称（假设 X_train 是训练数据）
feature_names = X_train.columns

# 将特征名称与权重拼接
feature_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importances
})

# 根据特征重要性进行筛选，保留重要性大于0.05的特征
filtered_features = feature_df[feature_df['importance'] > 0.05]

# 输出筛选后的特征
print("Filtered Features with Importance > 0.05:")
print(filtered_features)

# 如果需要提取筛选后的特征列表
filtered_feature_names = filtered_features['feature'].tolist()
print("Filtered Feature Names:")
print(filtered_feature_names)
```

### 详细步骤：

1.  **训练 LightGBM 模型**：
    -   在 `lgb_model.fit(X_train, y_train)` 之后，您会得到训练好的模型。
2.  **获取特征重要性**：
    -   使用 `booster_.feature_importance()` 获取每个特征的重要性。
    -   `importance_type='split'` 表示使用拆分次数来衡量特征重要性，`importance_type='gain'` 则表示使用增益来衡量特征的重要性。增益衡量特征在分裂时减少的目标函数的值。
3.  **筛选特征**：
    -   将特征名称和重要性值放到一个 DataFrame 中，筛选出重要性大于 0.05 的特征。
4.  **输出结果**：
    -   最后，你可以打印出筛选后的特征及其对应的权重。

### 结果解释：

-   **`importance_type='split'`**：每个特征出现在树的分裂中所占的次数。
-   **`importance_type='gain'`**：每个特征在树的分裂中所带来的增益，衡量特征对模型性能提升的贡献。

### 注意：

-   **`importance` 大于 0.05** 的特征意味着这些特征在模型训练中贡献较大，筛选出这些特征可以帮助你在特征选择过程中去除那些贡献较小的特征。
-   可以根据需要调整阈值，0.05 只是一个示例阈值。

希望这能帮助你有效筛选特征并理解其在模型中的重要性。如果有进一步问题，欢迎继续提问！