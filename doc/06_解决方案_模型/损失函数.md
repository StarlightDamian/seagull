我来为您提供15种常用的回归损失函数及其梯度计算。



```python
import numpy as np

class RegressionLosses:
    @staticmethod
    def mse_loss(y_true, y_pred):
        """
        1. Mean Squared Error Loss (MSE)
        L = (y_true - y_pred)^2
        gradient = -2(y_true - y_pred)
        """
        loss = np.mean(np.square(y_true - y_pred))
        gradient = -2 * (y_true - y_pred)
        return loss, gradient

    @staticmethod
    def mae_loss(y_true, y_pred):
        """
        2. Mean Absolute Error Loss (MAE)
        L = |y_true - y_pred|
        gradient = -sign(y_true - y_pred)
        """
        loss = np.mean(np.abs(y_true - y_pred))
        gradient = -np.sign(y_true - y_pred)
        return loss, gradient

    @staticmethod
    def huber_loss(y_true, y_pred, delta=1.0):
        """
        3. Huber Loss
        L = 0.5 * (y_true - y_pred)^2 if |y_true - y_pred| <= delta
        L = delta * |y_true - y_pred| - 0.5 * delta^2 otherwise
        """
        error = y_true - y_pred
        is_small_error = np.abs(error) <= delta
        
        loss = np.where(is_small_error,
                       0.5 * np.square(error),
                       delta * np.abs(error) - 0.5 * np.square(delta))
        
        gradient = np.where(is_small_error,
                          -error,
                          -delta * np.sign(error))
        
        return np.mean(loss), gradient

    @staticmethod
    def log_cosh_loss(y_true, y_pred):
        """
        4. Log-Cosh Loss
        L = log(cosh(y_pred - y_true))
        gradient = tanh(y_pred - y_true)
        """
        error = y_pred - y_true
        loss = np.mean(np.log(np.cosh(error)))
        gradient = np.tanh(error)
        return loss, gradient

    @staticmethod
    def quantile_loss(y_true, y_pred, quantile=0.5):
        """
        5. Quantile Loss
        L = max(quantile * error, (quantile-1) * error)
        where error = y_true - y_pred
        """
        error = y_true - y_pred
        loss = np.maximum(quantile * error, (quantile-1) * error)
        gradient = np.where(error > 0, -quantile, -(quantile-1))
        return np.mean(loss), gradient

    @staticmethod
    def fair_loss(y_true, y_pred, c=1.0):
        """
        6. Fair Loss
        L = c^2 * (|error|/c - log(1 + |error|/c))
        where error = y_true - y_pred
        """
        error = y_true - y_pred
        x = np.abs(error) / c
        loss = c**2 * (x - np.log(1 + x))
        gradient = -c * error / (c + np.abs(error))
        return np.mean(loss), gradient

    @staticmethod
    def cauchy_loss(y_true, y_pred, c=1.0):
        """
        7. Cauchy Loss
        L = c^2 * log(1 + (error/c)^2)
        gradient = 2*error/(c^2 + error^2)
        """
        error = y_true - y_pred
        loss = c**2 * np.log(1 + np.square(error/c))
        gradient = -2 * error / (c**2 + np.square(error))
        return np.mean(loss), gradient

    @staticmethod
    def charbonnier_loss(y_true, y_pred, epsilon=1e-3):
        """
        8. Charbonnier Loss (Pseudo-Huber Loss)
        L = sqrt((error)^2 + epsilon^2) - epsilon
        """
        error = y_true - y_pred
        loss = np.sqrt(np.square(error) + epsilon**2) - epsilon
        gradient = -error / np.sqrt(np.square(error) + epsilon**2)
        return np.mean(loss), gradient

    @staticmethod
    def wing_loss(y_true, y_pred, omega=10.0, epsilon=2.0):
        """
        9. Wing Loss
        L = omega * ln(1 + |error|/epsilon) if |error| < omega
        L = |error| - C if |error| >= omega
        where C = omega - omega * ln(1 + omega/epsilon)
        """
        error = y_true - y_pred
        C = omega - omega * np.log(1 + omega/epsilon)
        
        is_small_error = np.abs(error) < omega
        loss = np.where(is_small_error,
                       omega * np.log(1 + np.abs(error)/epsilon),
                       np.abs(error) - C)
        
        gradient = np.where(is_small_error,
                          -omega * np.sign(error)/(epsilon + np.abs(error)),
                          -np.sign(error))
        
        return np.mean(loss), gradient

    @staticmethod
    def geman_mcclure_loss(y_true, y_pred, sigma=1.0):
        """
        10. Geman-McClure Loss
        L = (error^2)/(2 * (sigma^2 + error^2))
        """
        error = y_true - y_pred
        squared_error = np.square(error)
        loss = (squared_error)/(2 * (sigma**2 + squared_error))
        gradient = -error * sigma**2 / np.power(sigma**2 + squared_error, 2)
        return np.mean(loss), gradient

    @staticmethod
    def welsch_loss(y_true, y_pred, c=1.0):
        """
        11. Welsch Loss
        L = (c^2/2) * (1 - exp(-(error/c)^2))
        """
        error = y_true - y_pred
        loss = (c**2/2) * (1 - np.exp(-np.square(error/c)))
        gradient = -error * np.exp(-np.square(error/c))
        return np.mean(loss), gradient

    @staticmethod
    def tukey_loss(y_true, y_pred, c=4.6851):
        """
        12. Tukey Biweight Loss
        L = (c^2/6) * (1 - (1 - (error/c)^2)^3) if |error| <= c
        L = c^2/6 if |error| > c
        """
        error = y_true - y_pred
        is_small_error = np.abs(error) <= c
        
        loss = np.where(is_small_error,
                       (c**2/6) * (1 - np.power(1 - np.square(error/c), 3)),
                       np.ones_like(error) * (c**2/6))
        
        gradient = np.where(is_small_error,
                          -error * np.power(1 - np.square(error/c), 2),
                          np.zeros_like(error))
        
        return np.mean(loss), gradient

    @staticmethod
    def adaptive_loss(y_true, y_pred, alpha=2.0, c=1.0):
        """
        13. Adaptive Loss
        L = (|error|/c)^alpha / alpha if alpha > 0
        L = log(1 + |error|/c) if alpha == 0
        """
        error = y_true - y_pred
        if alpha == 0:
            loss = np.log(1 + np.abs(error)/c)
            gradient = -np.sign(error)/(c + np.abs(error))
        else:
            loss = np.power(np.abs(error)/c, alpha) / alpha
            gradient = -np.sign(error) * np.power(np.abs(error)/c, alpha-1) / c
        
        return np.mean(loss), gradient

    @staticmethod
    def trimmed_loss(y_true, y_pred, ratio=0.2):
        """
        14. Trimmed Loss (only considers top (1-ratio) errors)
        L = MSE of top (1-ratio) errors
        """
        error = y_true - y_pred
        squared_error = np.square(error)
        k = int(len(error) * (1 - ratio))
        threshold = np.partition(squared_error, -k)[-k]
        
        mask = squared_error >= threshold
        loss = np.mean(squared_error[mask])
        gradient = np.where(mask, -2 * error, 0)
        
        return loss, gradient

    @staticmethod
    def gaussian_loss(y_true, y_pred, sigma=1.0):
        """
        15. Gaussian Loss
        L = 1 - exp(-0.5 * (error/sigma)^2)
        """
        error = y_true - y_pred
        loss = 1 - np.exp(-0.5 * np.square(error/sigma))
        gradient = -error * np.exp(-0.5 * np.square(error/sigma)) / sigma**2
        return np.mean(loss), gradient

# 测试代码
def test_losses():
    # 创建示例数据
    y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
    y_pred = np.array([1.1, 2.2, 2.8, 4.2, 4.9])
    
    losses = RegressionLosses()
    
    # 测试所有损失函数
    loss_functions = [
        ('MSE Loss', losses.mse_loss),
        ('MAE Loss', losses.mae_loss),
        ('Huber Loss', losses.huber_loss),
        ('Log-Cosh Loss', losses.log_cosh_loss),
        ('Quantile Loss', losses.quantile_loss),
        ('Fair Loss', losses.fair_loss),
        ('Cauchy Loss', losses.cauchy_loss),
        ('Charbonnier Loss', losses.charbonnier_loss),
        ('Wing Loss', losses.wing_loss),
        ('Geman-McClure Loss', losses.geman_mcclure_loss),
        ('Welsch Loss', losses.welsch_loss),
        ('Tukey Loss', losses.tukey_loss),
        ('Adaptive Loss', losses.adaptive_loss),
        ('Trimmed Loss', losses.trimmed_loss),
        ('Gaussian Loss', losses.gaussian_loss)
    ]
    
    print("Testing all loss functions with example data:")
    print("y_true:", y_true)
    print("y_pred:", y_pred)
    print("\nResults:")
    
    for name, loss_fn in loss_functions:
        loss, gradient = loss_fn(y_true, y_pred)
        print(f"\n{name}:")
        print(f"Loss: {loss:.6f}")
        print(f"Gradient shape: {gradient.shape}")

if __name__ == "__main__":
    test_losses()

```

这些损失函数各有特点和适用场景：

1. **MSE Loss**
   - 最常用的损失函数
   - 对异常值敏感
   - 梯度计算简单

2. **MAE Loss**
   - 对异常值不敏感
   - 在零点不可导
   - 适合有噪声的数据

3. **Huber Loss**
   - MSE和MAE的结合
   - 参数delta控制敏感度
   - 对异常值稳健

4. **Log-Cosh Loss**
   - 类似Huber但处处可导
   - 计算稳定性好
   - 梯度平滑

5. **Quantile Loss**
   - 用于分位数回归
   - 可以预测分布
   - 适合不对称误差代价

6. **Fair Loss**
   - 对异常值稳健
   - 处处可导
   - 梯度变化平滑

7. **Cauchy Loss**
   - 强烈抑制异常值
   - 非凸损失函数
   - 适合重尾数据

8. **Charbonnier Loss**
   - Huber Loss的平滑版本
   - 处处可导
   - 计算效率高

9. **Wing Loss**
   - 关注小误差
   - 线性处理大误差
   - 适合精确回归

10. **Geman-McClure Loss**
    - 强力抑制异常值
    - 梯度平滑
    - 适合噪声数据

11. **Welsch Loss**
    - 对异常值稳健
    - 快速收敛
    - 梯度变化平缓

12. **Tukey Loss**
    - 完全忽略大误差
    - 适合严重污染数据
    - 需要调参

13. **Adaptive Loss**
    - 可调整形状
    - 适应不同场景
    - 参数敏感

14. **Trimmed Loss**
    - 忽略部分大误差
    - 简单有效
    - 参数直观

15. **Gaussian Loss**
    - 基于概率模型
    - 梯度平滑
    - 计算稳定

选择损失函数时需要考虑：
1. 数据分布特点
2. 异常值敏感度要求
3. 计算效率需求
4. 梯度稳定性要求

需要针对特定应用场景调整某个损失函数吗？