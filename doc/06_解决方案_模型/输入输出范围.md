在使用 LightGBM 进行回归时，默认的输出并不会限制在某个特定的范围（比如 `[-1, 1]`）。但是，你可以通过以下几种方法来确保回归的预测值在这个范围内：

### 方法一：输出后处理

最简单且直接的方法是训练模型后，在预测结果上进行后处理，限制输出值在 `[-1, 1]` 之间。你可以使用 `numpy.clip()` 函数来实现这一点。

```python
import numpy as np
import lightgbm as lgb

# 假设你已经训练好了 LightGBM 回归模型
# model = lgb.train(...)

# 对于预测结果，使用 clip 函数将预测值限制在 [-1, 1] 之间
y_pred = model.predict(X_test)
y_pred_clipped = np.clip(y_pred, -1, 1)
```

### 方法二：自定义目标函数（Target Function）

你可以通过在 LightGBM 中自定义目标函数（objective function）来确保回归值在 `[-1, 1]` 范围内。自定义目标函数的一个常见做法是将预测值映射到一个更宽的区间，训练过程中通过模型优化目标函数，然后再将输出值缩放回 `[-1, 1]`。

但是，这种方法的实现稍显复杂，通常需要用到数学变换（如 Sigmoid 函数或其它范围映射）来确保预测结果保持在该范围内。

例如，假设你使用 Sigmoid 函数将预测映射到 `[-1, 1]`：

```python
import lightgbm as lgb
import numpy as np

# 自定义目标函数，使用Sigmoid函数来映射输出
def sigmoid_mapping(y_pred):
    return 2 / (1 + np.exp(-y_pred)) - 1  # 将预测值映射到 [-1, 1]

# 自定义目标函数
def custom_objective(y_true, y_pred):
    # 在这里你可以定义你的损失函数，比如 MSE 等
    grad = 2 * (sigmoid_mapping(y_pred) - y_true)  # 梯度
    hess = np.ones_like(y_pred)  # Hessian
    return grad, hess

# 假设你已经加载了训练数据 X_train, y_train
# 创建模型
model = lgb.LGBMRegressor(objective=custom_objective)

# 训练模型
model.fit(X_train, y_train)
```

### 方法三：使用 `sigmoid` 输出并限制范围

另一种方法是使用 `sigmoid` 函数将回归输出值映射到 `[-1, 1]` 范围。训练时，你可以设置 LightGBM 输出一个线性值，然后在预测时通过 `sigmoid` 函数限制值的范围。

这种方法通常需要额外的后处理步骤。

```python
from sklearn.preprocessing import MinMaxScaler

# 假设y_pred是你模型的输出
scaler = MinMaxScaler(feature_range=(-1, 1))
y_pred_scaled = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()
```

### 方法四：使用激活函数（如 Tanh）约束输出范围

另一种方法是使用 `Tanh` 激活函数来将回归输出限制到 `[-1, 1]` 范围。`Tanh` 本身输出的值就位于这个区间。

你可以在模型预测时应用 `Tanh` 函数：

```python
import numpy as np

# 假设你已经训练好了 LightGBM 模型
# y_pred = model.predict(X_test)

# 对预测值应用 Tanh 函数
y_pred_tanh = np.tanh(y_pred)  # Tanh 会将值映射到 [-1, 1] 范围
```

### 总结

-   最简单且常用的方式是对回归预测值应用后处理，使用 `numpy.clip()` 将预测值限制在 `[-1, 1]` 范围内。
-   如果你希望从模型训练中就考虑输出范围，可以考虑自定义目标函数或应用激活函数（如 `Tanh` 或 `Sigmoid`）来约束输出范围。

一般来说，第一种方法（后处理）最为简单有效，适用于大多数场景。